{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19955cc4-fde4-405c-ab6c-313e82fa8947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing index es_clients_with_duplicates\n",
      "Created index es_clients_with_duplicates with custom settings and mappings\n",
      "Data uploaded to index es_clients_with_duplicates\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch_dsl import Search, Q, A\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "## All information is randomly generated, contains no PII whatsoever\n",
    "\n",
    "# Import CSV to df\n",
    "df_clients_with_duplicates = pd.read_csv('csv/clients_with_duplicates.csv')\n",
    "\n",
    "# Replace all NaN values with an empty string\n",
    "df_clients_with_duplicates.fillna('', inplace=True)\n",
    "\n",
    "# Reduce number of entries\n",
    "df_clients_with_duplicates = df_clients_with_duplicates.head(3000)\n",
    "\n",
    "def setup_elasticsearch_index(df, index_name):\n",
    "    # Initialize Elasticsearch client\n",
    "    es = Elasticsearch(\n",
    "        'https://elastic:9200',\n",
    "        basic_auth=('elastic', 'password'),\n",
    "        verify_certs=False,\n",
    "        ssl_show_warn=False\n",
    "    )\n",
    "    # Delete the index if it exists\n",
    "    if es.indices.exists(index=index_name):\n",
    "        es.indices.delete(index=index_name)\n",
    "        print(f\"Deleted existing index {index_name}\")\n",
    "\n",
    "    # Create the index with optimized settings for client fields analysis\n",
    "    settings = {\n",
    "        \"settings\": {\n",
    "            \"analysis\": {\n",
    "                \"analyzer\": {\n",
    "                    \"name_folding_analyzer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"standard\",  # separate words by a space character\n",
    "                        \"filter\": [\"lowercase\", \"asciifolding\"]\n",
    "                    },\n",
    "                    \"legal_folding_analyzer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"standard\",\n",
    "                        \"filter\": [\"lowercase\", \"asciifolding\"],\n",
    "                        \"char_filter\": [\"hyphen_removal\"]\n",
    "                    }\n",
    "                },\n",
    "                \"normalizer\": {\n",
    "                    \"name_folding_normalizer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"filter\": [\"lowercase\", \"asciifolding\"]\n",
    "                    },\n",
    "                    \"legal_folding_normalizer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"filter\": [\"lowercase\", \"asciifolding\"],\n",
    "                        \"char_filter\": [\"hyphen_removal\"]\n",
    "                    }\n",
    "                },\n",
    "                \"char_filter\": {\n",
    "                    \"hyphen_removal\": {\n",
    "                        \"type\": \"pattern_replace\",\n",
    "                        \"pattern\": \"[\\\\-\\\\s]\",  # regex targets both hyphens and whitespace characters\n",
    "                        \"replacement\": \"\"\n",
    "                    }\n",
    "                }                \n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"name\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"analyzer\": \"name_folding_analyzer\",\n",
    "                    \"fields\": {\n",
    "                        \"keyword\": {\n",
    "                            \"type\": \"keyword\",\n",
    "                            \"ignore_above\": 256\n",
    "                        },\n",
    "                        \"folded\": {\n",
    "                            \"type\": \"keyword\",\n",
    "                            \"normalizer\": \"name_folding_normalizer\"\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"legal\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"analyzer\": \"legal_folding_analyzer\",\n",
    "                    \"fields\": {\n",
    "                        \"keyword\": {\n",
    "                            \"type\": \"keyword\",\n",
    "                            \"ignore_above\": 256\n",
    "                        },\n",
    "                        \"folded\": {\n",
    "                            \"type\": \"keyword\",\n",
    "                            \"normalizer\": \"legal_folding_normalizer\"\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"email1\": {\n",
    "                    \"type\": \"keyword\",\n",
    "                    \"ignore_above\": 256\n",
    "                },\n",
    "                \"email2\": {\n",
    "                    \"type\": \"keyword\",\n",
    "                    \"ignore_above\": 256\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    es.indices.create(index=index_name, body=settings)\n",
    "    print(f\"Created index {index_name} with custom settings and mappings\")\n",
    "\n",
    "    # Create an iterable of actions to be executed in bulk\n",
    "    actions = [\n",
    "        {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": str(record['id']),\n",
    "            \"_source\": record,\n",
    "        }\n",
    "        for record in df.to_dict(orient='records')\n",
    "    ]\n",
    "    \n",
    "    # Perform bulk insert\n",
    "    helpers.bulk(es, actions)\n",
    "    # Refresh the index to make the changes searchable\n",
    "    es.indices.refresh(index=index_name)\n",
    "    print(f\"Data uploaded to index {index_name}\")\n",
    "\n",
    "setup_elasticsearch_index(df_clients_with_duplicates, 'es_clients_with_duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72b341a9-353e-4818-af86-bd0985dd9e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 14:27:27,147 - INFO - Analyzed 3000 client profiles in 0.70 seconds.\n",
      "2024-10-20 14:27:27,147 - INFO - Found 1041 Client IDs with potential duplicates.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>email1</th>\n",
       "      <th>email2</th>\n",
       "      <th>legal</th>\n",
       "      <th>matched_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45301</td>\n",
       "      <td>Nina Fuller</td>\n",
       "      <td>24/10/24</td>\n",
       "      <td>nin_ful@yahoo.123</td>\n",
       "      <td>nin_ful@yahoo.123</td>\n",
       "      <td>Y1079501</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50301</td>\n",
       "      <td>Nìna Fullér</td>\n",
       "      <td>24/10/19</td>\n",
       "      <td>nin_ful@gmail.xyz</td>\n",
       "      <td></td>\n",
       "      <td>Y-1079501</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45173</td>\n",
       "      <td>Grace Banks</td>\n",
       "      <td>01/05/22</td>\n",
       "      <td>gracebanks77@hotmail.abc</td>\n",
       "      <td>gbanks@gmail.xyz</td>\n",
       "      <td>2065463418</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50173</td>\n",
       "      <td>Gracé Banks</td>\n",
       "      <td>01/05/20</td>\n",
       "      <td>gra_ban@gmx.bbb</td>\n",
       "      <td></td>\n",
       "      <td>0578890674</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49836</td>\n",
       "      <td>Lèónel Keller</td>\n",
       "      <td>16/05/18</td>\n",
       "      <td>leo_kel@hotmail.abc</td>\n",
       "      <td>lkeller@gmx.bbb</td>\n",
       "      <td>X-6102347</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>47782</td>\n",
       "      <td>Ira Blanchard</td>\n",
       "      <td>11/12/21</td>\n",
       "      <td>ira_bla@yahoo.123</td>\n",
       "      <td>iblanchard@hotmail.abc</td>\n",
       "      <td>X5056099</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>46734</td>\n",
       "      <td>Joey Harrison</td>\n",
       "      <td>15/10/24</td>\n",
       "      <td>joe_har@hotmail.abc</td>\n",
       "      <td>jharrison@yahoo.123</td>\n",
       "      <td>Y8571854</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>47449</td>\n",
       "      <td>Gwendolyn Jackson</td>\n",
       "      <td>27/05/22</td>\n",
       "      <td>gjackson@gmail.xyz</td>\n",
       "      <td>jharrison@yahoo.123</td>\n",
       "      <td>Z7981051</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>43540</td>\n",
       "      <td>Magnus Garner</td>\n",
       "      <td>26/04/22</td>\n",
       "      <td>magnusgarner45@outlook.456</td>\n",
       "      <td>kmclean@gmx.bbb</td>\n",
       "      <td>X7274840</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>44637</td>\n",
       "      <td>Kaylie McLean</td>\n",
       "      <td>16/01/24</td>\n",
       "      <td>kmclean@gmx.bbb</td>\n",
       "      <td>kay_mcl@523344123.net</td>\n",
       "      <td>X9718033</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1042 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id               name      date                      email1  \\\n",
       "0     45301        Nina Fuller  24/10/24           nin_ful@yahoo.123   \n",
       "1     50301        Nìna Fullér  24/10/19           nin_ful@gmail.xyz   \n",
       "2     45173        Grace Banks  01/05/22    gracebanks77@hotmail.abc   \n",
       "3     50173        Gracé Banks  01/05/20             gra_ban@gmx.bbb   \n",
       "4     49836      Lèónel Keller  16/05/18         leo_kel@hotmail.abc   \n",
       "...     ...                ...       ...                         ...   \n",
       "1037  47782      Ira Blanchard  11/12/21           ira_bla@yahoo.123   \n",
       "1038  46734      Joey Harrison  15/10/24         joe_har@hotmail.abc   \n",
       "1039  47449  Gwendolyn Jackson  27/05/22          gjackson@gmail.xyz   \n",
       "1040  43540      Magnus Garner  26/04/22  magnusgarner45@outlook.456   \n",
       "1041  44637      Kaylie McLean  16/01/24             kmclean@gmx.bbb   \n",
       "\n",
       "                      email2       legal  matched_set  \n",
       "0          nin_ful@yahoo.123    Y1079501            1  \n",
       "1                              Y-1079501            1  \n",
       "2           gbanks@gmail.xyz  2065463418            2  \n",
       "3                             0578890674            2  \n",
       "4            lkeller@gmx.bbb   X-6102347            3  \n",
       "...                      ...         ...          ...  \n",
       "1037  iblanchard@hotmail.abc    X5056099          521  \n",
       "1038     jharrison@yahoo.123    Y8571854          522  \n",
       "1039     jharrison@yahoo.123    Z7981051          522  \n",
       "1040         kmclean@gmx.bbb    X7274840          523  \n",
       "1041   kay_mcl@523344123.net    X9718033          523  \n",
       "\n",
       "[1042 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a cross-comparison among all documents in the index for any given field\n",
    "\n",
    "# Clear previous logging handlers if any\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Configure logging to show the time, level, and message without the logger's name\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def find_and_display_duplicates():\n",
    "    start_time = time.time()  # Start timing the analysis\n",
    "    \n",
    "    es = Elasticsearch(\n",
    "        'https://elastic:9200',\n",
    "        basic_auth=('elastic', 'password'),\n",
    "        verify_certs=False,\n",
    "        ssl_show_warn=False\n",
    "    )\n",
    "\n",
    "    # Count index documents for logs\n",
    "    total_docs = es.count(index='es_clients_with_duplicates')['count']\n",
    "    \n",
    "    # Setup aggregation queries for names and legal fields\n",
    "    s = Search(using=es, index='es_clients_with_duplicates').source(False).params(size=0)\n",
    "    s.aggs.bucket('by_name', 'terms', field='name.folded', min_doc_count=2, size=10000)\n",
    "    s.aggs.bucket('by_legal', 'terms', field='legal.folded', min_doc_count=2, size=10000)\n",
    "    response = s.execute()\n",
    "\n",
    "    # Extract keys from buckets\n",
    "    folded_names = {bucket.key for bucket in response.aggregations.by_name.buckets}\n",
    "    folded_legals = {bucket.key for bucket in response.aggregations.by_legal.buckets}\n",
    "\n",
    "    data = []\n",
    "    current_matched_set = 1\n",
    "    processed_doc_ids = set()\n",
    "\n",
    "    # Function to fetch and assign matched_set for documents\n",
    "    def fetch_and_assign(query, matched_set):\n",
    "        search = Search(using=es, index='es_clients_with_duplicates').query(query)\n",
    "        search = search.source(['name', 'date', 'legal', 'email1', 'email2'])  # Added email fields\n",
    "        results = search.execute()\n",
    "\n",
    "        assigned_data = []\n",
    "        for hit in results:\n",
    "            doc_id = hit.meta.id\n",
    "            if doc_id not in processed_doc_ids:\n",
    "                processed_doc_ids.add(doc_id)\n",
    "                assigned_data.append({\n",
    "                    \"id\": doc_id,\n",
    "                    \"name\": hit.name,\n",
    "                    \"date\": hit.date,\n",
    "                    \"email1\": hit.email1,\n",
    "                    \"email2\": hit.email2,\n",
    "                    \"legal\": hit.legal,\n",
    "                    \"matched_set\": matched_set\n",
    "                })\n",
    "\n",
    "        return assigned_data\n",
    "\n",
    "    # Process and assign matched_set for name and legal\n",
    "    for field, values in [('name.folded', folded_names), ('legal.folded', folded_legals)]:\n",
    "        for value in values:\n",
    "            if value:  # Check for empty values to avoid processing them\n",
    "                query = Q('term', **{field: value})\n",
    "                results = fetch_and_assign(query, current_matched_set)\n",
    "                if results:\n",
    "                    data.extend(results)\n",
    "                    current_matched_set += 1\n",
    "\n",
    "    # Collect and process emails for duplicate detection\n",
    "    s = Search(using=es, index='es_clients_with_duplicates').source(['email1', 'email2'])\n",
    "    email_counts = {}\n",
    "    for hit in s.scan():\n",
    "        if hit.email1:\n",
    "            email_counts[hit.email1] = email_counts.get(hit.email1, 0) + 1\n",
    "        if hit.email2 and hit.email2 != hit.email1:\n",
    "            email_counts[hit.email2] = email_counts.get(hit.email2, 0) + 1\n",
    "\n",
    "    common_emails = {email for email, count in email_counts.items() if count > 1}\n",
    "    for email in common_emails:\n",
    "        query = Q('bool', should=[Q('term', email1=email), Q('term', email2=email)])\n",
    "        results = fetch_and_assign(query, current_matched_set)\n",
    "        if results:\n",
    "            data.extend(results)\n",
    "            current_matched_set += 1\n",
    "\n",
    "    # Create and return the DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    if not df.empty:\n",
    "        df = df.sort_values(by=['matched_set'])\n",
    "\n",
    "    logging.info(f\"Analyzed {total_docs} client profiles in {time.time() - start_time:.2f} seconds.\")\n",
    "    logging.info(f\"Found {len(df)-1} Client IDs with potential duplicates.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Display results\n",
    "duplicates_df = find_and_display_duplicates()\n",
    "duplicates_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
